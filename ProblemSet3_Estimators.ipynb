{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Estimators\n",
    "\n",
    "## MLE Proof\n",
    "\n",
    "Suppose we have a random sample $X_1, X_2, \\dots X_N$ whose assumed probability distribution depends on some unknown parameter $\\theta$. The observed values of the sample are $x_1, x_2, \\dots x_N$. We want to find an estimator $u(X_1, X_2, \\dots X_N)$ such that $u(x_1, x_2, \\dots x_N)$ is a \"good\" estimate of $\\theta$. It seems reasonable that such an estimate of the unknown parameter $\\theta$ would be the value of $u$ that maximises the probability, or the likelihood, of getting the actual data we observed.\n",
    "\n",
    "If the probability density function of each $X_i$ is $f(x_i; \\theta)$, the joint probability mass (or density) function of $X_1, X_2, \\dots X_N$ is\n",
    "\n",
    "$$L(\\theta) = P(X_1 = x_1, X_2 = x_2, \\dots X_N = x_N) = \\prod_i^N f(x_i; \\theta)$$\n",
    "\n",
    "Assuming that the $X_i$ are independent Bernoulli random variables with unknown parameter $p$, the probability mass function of each $X_i$ is\n",
    "\n",
    "$$f(x_i; p) = p^{x_i} (1-p)^{1-x_i}$$\n",
    "\n",
    "In order to maximise the function, we need to find the $p$ that maximises the likelihood $L(p)$. To make the differentiation easier, we note that the value of $p$ that maximises $\\ln \\left[L(p)\\right]$ will also maximise $L(p)$.\n",
    "\n",
    "$$\\ln[L(p)] = \\ln \\left[ \\prod_i p^{x_i} (1-p)^{1-x_i} \\right] = \\sum_i \\left[x_i \\ln(p) + (1-x_i) \\ln(1-p) \\right]$$\n",
    "\n",
    "Take the first derivative with respect to $p$, and set it to zero:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial p} \\ln[L(p)] = \\sum_i \\left[\\frac{x_i}{p} - \\frac{1-x_i}{1-p} \\right] = 0$$\n",
    "\n",
    "Multiply both sides by $p (1-p)$:\n",
    "\n",
    "$$ \\sum_i \\left[(1-p) x_i - p (1 - x_i)\\right] = \\sum_i (x_i - p) = 0$$\n",
    "\n",
    "$$\\sum_i x_i - N p = 0$$\n",
    "\n",
    "Thus, we finally get\n",
    "\n",
    "$$p = \\frac{1}{N} \\sum_i x_i$$\n",
    "\n",
    "Source: https://onlinecourses.science.psu.edu/stat414/node/191"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize and scale data\n",
    "\n",
    "Suppose you have a set of data $x_1, x_2, \\dots x_N$ with mean 5, standard deviation 4 and variance 16. If $x_i =$ 9, what is its standard score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "xi = 9\n",
    "mu = 5\n",
    "sd = 4\n",
    "\n",
    "print((xi-mu)/sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's multiply every data point by 1.5 and calculate the new mean, variance and standard deviation. Also calculate the new standard score $z$ of the point we considered earlier.\n",
    "\n",
    "$$\\mu' = \\sum_i x_i'= \\sum_i (1.5 x_i) = 1.5 \\mu$$\n",
    "\n",
    "$$\\sigma'^2 = \\frac{1}{N} \\sum_i (1.5 x_i - \\mu')^2 = \\frac{1}{N} \\sum_i (1.5 x_i - 1.5 \\mu)^2 = 2.25 \\sigma^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "xi = 1.5*xi\n",
    "mu = 1.5*mu\n",
    "sd = 1.5*sd\n",
    "\n",
    "print(xi)\n",
    "print((xi-mu)/sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scatter plot spread\n",
    "\n",
    "If we have the following plot of adult height vs child height,\n",
    "\n",
    "![](scatterplot.png)\n",
    "\n",
    "which of two variables has the largest variance?\n",
    "\n",
    "*Answer: adult height - look at how the data is spread out with respect to the mean for each variable. Of course, we're assuming that the scales on the axes are comparable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram averages\n",
    "\n",
    "In the following histogram, what are the mean, median and mode?\n",
    "\n",
    "![](histogram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "mean = (1 + 2*2 + 3*3 + 2*4 + 5 + 6 + 11)/(1 + 2 + 3 + 2 + 1 + 1 + 1)\n",
    "print(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value 3 is both the median and the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Variance\n",
    "\n",
    "Consider a case where a number can be 0 or 1 with probability 0.5 for each. What is its variance? Note that this is the same as figuring out the variance of two data points, one of which is 0 and the other is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.array([0, 1])\n",
    "\n",
    "print(np.var(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask what we expect the variance to be when we generate a couple of data points, e.g. by flipping coins. The possibilities are:\n",
    "\n",
    "- 0 0\n",
    "- 0 1\n",
    "- 1 0\n",
    "- 1 1\n",
    "\n",
    "For each case compute the variance, and then find the average of the variances, which is what we call **expected variance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.25\n",
      "0.25\n",
      "0.0\n",
      "0.125\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "exvar = 0\n",
    "for row in data:\n",
    "    ivar = np.var(row)\n",
    "    exvar += ivar\n",
    "    print(ivar)\n",
    "print(exvar/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the actual variance we need to multiply the expected variance by 2 (a **correction factor**). More generally, this correction factor is $n/(n-1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median\n",
    "\n",
    "If we have an even number of data points in our sample, there is no midpoint element. We have two options for defining the median of the sample:\n",
    "\n",
    "1. choose either of the two central elements, or\n",
    "2. use the average of the two central elements.\n",
    "\n",
    "The first option is called the **in-data median** because it returns a value which is present in the sample; the problem with it is that it is not unique. This is overcome by using the second option, however it returns a value which is not in the sample. The second option is what `np.median()` uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.0\n"
     ]
    }
   ],
   "source": [
    "data = np.array([5, 9, 15, 25])\n",
    "\n",
    "print(np.median(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use option #1, we modify the `median` function created in unit #16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def median(data):\n",
    "    sdata = sorted(data)\n",
    "    \n",
    "    # Find the midpoint, or else choose one of the two\n",
    "    # central elements at random\n",
    "    midpoint = (len(data)-1)/2\n",
    "    results = [sdata[midpoint], sdata[midpoint+1]]\n",
    "    \n",
    "    if (len(data) % 2 == 0):\n",
    "        return random.choice(results)\n",
    "    else:\n",
    "        return results[0]\n",
    "\n",
    "print(median(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental mean\n",
    "\n",
    "Write a function `mean()` thatn can take a known mean of existing data, a count of the amount of data and a new value, and recalculate the new mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "def mean(oldmean, n, x):\n",
    "    return (oldmean*n + x)/(n+1)\n",
    "\n",
    "currentmean = 10\n",
    "currentcount = 5\n",
    "new = 4\n",
    "\n",
    "print(mean(currentmean, currentcount, new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood challenge\n",
    "\n",
    "Imagine we have a die with an arbitrary number of sides; each side can be labeled anything we want, and the die can be fair or loaded. This die is described by the dictionary `dist` below, which can be thought of as a distribution of values (or more formally a probability distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dist  = {'A':0.2, 'B':0.2, 'C':0.2, 'D':0.2, 'E':0.2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function called `likelihood()` that takes some data and the distribution and returns the likelihood of the data given the distribution, that is, the probability of getting the specific set of rolls in the given order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct\n",
      "Correct\n",
      "Correct\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "def likelihood(dist, data):\n",
    "    # To get the likelihood of a set of numbers, we just need to multiply in\n",
    "    # the probability of seeing every single data point\n",
    "    result = 1\n",
    "    for x in data:\n",
    "        result *= dist[x]\n",
    "    return result\n",
    "\n",
    "tests = [((dist, 'ABCEDDECAB'), 1.024e-07),\n",
    "         (({'Good':0.6, 'Bad':0.2, 'Indifferent':0.2},['Good','Bad','Indifferent','Good','Good','Bad']),0.001728),\n",
    "         (({'Z':0.6, 'X':0.333, 'Y':0.067}, 'ZXYYZXYXYZY'), 1.07686302456e-08),\n",
    "         (({'Z':0.6, 'X':0.233, 'Y':0.067, 'W':0.1}, 'WXYZYZZZZW'), 8.133206112e-07)]\n",
    "\n",
    "for t, l in tests:\n",
    "    if abs(likelihood(*t)/l-1) < 0.01: print('Correct')\n",
    "    else: print('Incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:introstats]",
   "language": "python",
   "name": "conda-env-introstats-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
